{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eea62c-be6a-45e6-9b69-99cbf7524c6f",
   "metadata": {},
   "source": [
    "![](./lab%20header%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66a82e-3c64-4e56-8d7c-b15835e618a7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h3>Experiment No. 08</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c407c38-a93b-428d-ad91-6865b43970f1",
   "metadata": {},
   "source": [
    "<img src=\"./Student%20Information.png\" style=\"width: 100%;\" alt=\"Student Information\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eedca-8bc9-493b-bfbd-681db5fbb355",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>AIM</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a2524-23ae-4df1-af53-9c834d2f54f4",
   "metadata": {},
   "source": [
    "**Study Assignment of Information Retrieval Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f1b94-260f-414c-8c1b-0e1b71b99e04",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>Theory/Procedure/Algorithm</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d072d40-3d52-4640-bdb1-b3e185f63f28",
   "metadata": {},
   "source": [
    "Information retrieval (IR) refers to the process of obtaining relevant information from a large repository of unstructured data. The primary goal of IR is to provide users with results that satisfy their search queries as accurately and quickly as possible. In this experiment, we explore key IR techniques, their implementation, and evaluate their effectiveness using various metrics. IR plays a significant role in search engines, recommendation systems, and document management applications.\n",
    "\n",
    "There are several core techniques used in information retrieval systems:\n",
    "\n",
    "1. **Boolean Retrieval Model**: This is one of the earliest and simplest retrieval models where queries are expressed using Boolean operators (AND, OR, NOT). It retrieves documents that either satisfy or do not satisfy the query conditions. This method is very rigid and either returns too many or too few results.\n",
    "\n",
    "2. **Vector Space Model (VSM)**: In this model, documents and queries are represented as vectors in a multi-dimensional space. The relevance between a document and a query is calculated based on the cosine similarity between the vectors. VSM handles partial matches and ranks documents based on their relevance to the query.\n",
    "\n",
    "3. **Probabilistic Retrieval Model**: This model estimates the probability of a document being relevant to a query. The most well-known example is the BM25 ranking function, which improves relevance scoring by considering term frequency, inverse document frequency, and document length.\n",
    "\n",
    "4. **Latent Semantic Indexing (LSI)**: LSI deals with the limitations of traditional keyword-based models by discovering latent relationships between terms. It applies singular value decomposition (SVD) to a term-document matrix to reduce its dimensionality, making it easier to retrieve conceptually related documents.\n",
    "\n",
    "5. **TF-IDF (Term Frequency-Inverse Document Frequency)**: This technique ranks documents based on the frequency of query terms. The relevance score is determined by two factors:\n",
    "\n",
    "    - **Term Frequency (TF)**: The number of times a term appears in a document.\n",
    "    - **Inverse Document Frequency (IDF)**: It reflects how rare a term is across all documents. The less frequent a term, the higher its significance in identifying relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ffc6f-90dc-4ae7-80c7-c15103979a98",
   "metadata": {},
   "source": [
    "##### Steps:\n",
    "\n",
    "1. **Dataset**: The 20 Newsgroups dataset can be accessed from Scikit-learn's datasets module, and it is freely available for use. Here’s the link to the dataset: 20 Newsgroups Dataset.\n",
    "\n",
    "2. **Implementation**: I will use the following IR models:\n",
    "- **TF-IDF Vectorizer** (for vector space model)\n",
    "- **BM25 (for probabilistic** model)\n",
    "- **Boolean Retrieval** (with binary vectors)\n",
    "\n",
    "3. **Performance Metrics**: Precision, Recall, F1 Score, MAP, and MRR will be calculated based on retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6adf609-a8e3-409d-bd21-fb74eba96386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_vsm': 1.0, 'recall_vsm': 0.5, 'f1_vsm': 0.6666666666666666, 'precision_bm25': 1.0, 'recall_bm25': 0.5, 'f1_bm25': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['sci.space', 'comp.graphics', 'rec.autos', 'talk.politics.guns']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
    "data = newsgroups.data\n",
    "target = newsgroups.target\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the train and test data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_data)\n",
    "tfidf_test = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Define a sample query (from test data)\n",
    "query = test_data[0]\n",
    "query_vector = tfidf_vectorizer.transform([query])\n",
    "\n",
    "# Cosine similarity between the query and train data (VSM)\n",
    "cosine_sim = cosine_similarity(query_vector, tfidf_train).flatten()\n",
    "\n",
    "# BM25 implementation with rank_bm25\n",
    "tokenized_corpus = [doc.split() for doc in train_data]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "query_tokenized = query.split()\n",
    "bm25_scores = bm25.get_scores(query_tokenized)\n",
    "\n",
    "# For the purpose of comparison, rank the documents using the cosine similarity scores\n",
    "top_n_vsm = np.argsort(cosine_sim)[::-1][:10]  # Top 10 docs for VSM\n",
    "top_n_bm25 = np.argsort(bm25_scores)[::-1][:10]  # Top 10 docs for BM25\n",
    "\n",
    "# Let's compute precision, recall, and F1 Score for a binary relevance task (whether the predicted category matches the query category)\n",
    "true_category = test_target[0]\n",
    "\n",
    "# For VSM (Binary relevance based on whether the predicted doc category matches the true query category)\n",
    "vsm_pred_categories = [train_target[idx] for idx in top_n_vsm]\n",
    "vsm_relevant = [1 if cat == true_category else 0 for cat in vsm_pred_categories]\n",
    "\n",
    "# For BM25 (Binary relevance based on whether the predicted doc category matches the true query category)\n",
    "bm25_pred_categories = [train_target[idx] for idx in top_n_bm25]\n",
    "bm25_relevant = [1 if cat == true_category else 0 for cat in bm25_pred_categories]\n",
    "\n",
    "# Precision, Recall, F1 for VSM\n",
    "precision_vsm = precision_score([1] * len(vsm_relevant), vsm_relevant, zero_division=0)\n",
    "recall_vsm = recall_score([1] * len(vsm_relevant), vsm_relevant, zero_division=0)\n",
    "f1_vsm = f1_score([1] * len(vsm_relevant), vsm_relevant, zero_division=0)\n",
    "\n",
    "# Precision, Recall, F1 for BM25\n",
    "precision_bm25 = precision_score([1] * len(bm25_relevant), bm25_relevant, zero_division=0)\n",
    "recall_bm25 = recall_score([1] * len(bm25_relevant), bm25_relevant, zero_division=0)\n",
    "f1_bm25 = f1_score([1] * len(bm25_relevant), bm25_relevant, zero_division=0)\n",
    "\n",
    "# Return results\n",
    "result = {\n",
    "    \"precision_vsm\": precision_vsm,\n",
    "    \"recall_vsm\": recall_vsm,\n",
    "    \"f1_vsm\": f1_vsm,\n",
    "    \"precision_bm25\": precision_bm25,\n",
    "    \"recall_bm25\": recall_bm25,\n",
    "    \"f1_bm25\": f1_bm25\n",
    "}\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958c385-14b3-494c-844f-37b343fef648",
   "metadata": {},
   "source": [
    "The results indicate the following for both the Vector Space Model (VSM) and BM25 methods when retrieving documents for a given query:\n",
    "\n",
    "1. **Precision**:\n",
    "Precision (1.0) for both VSM and BM25 indicates that all the top 10 documents retrieved were relevant to the query. This means that there were no false positives—every document retrieved was in the correct category.\n",
    "\n",
    "2. **Recall**:\n",
    "Recall (0.5) for both VSM and BM25 shows that the models retrieved only 50% of the relevant documents available in the dataset for that query. In other words, while the retrieved documents were relevant, the models failed to find all the relevant documents (some relevant documents were missed).\n",
    "\n",
    "3. **F1 Score**:\n",
    "F1 Score (0.67) is the harmonic mean of precision and recall. Since precision was high (1.0) but recall was moderate (0.5), the F1 score reflects this balance, sitting at 0.67. It shows that while the system is precise in retrieving relevant documents, it could improve in terms of covering all relevant documents.\n",
    "\n",
    "**Summary**:\n",
    "- High precision (1.0) indicates that the methods are good at not retrieving irrelevant documents.\n",
    "- Moderate recall (0.5) shows that the methods are not retrieving all relevant documents.\n",
    "- Both VSM and BM25 performed similarly in this test, but the models could be further optimized to improve recall, potentially increasing the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab992f17-f9a0-477d-a948-d1307e838670",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>CONCLUSION</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2814c1-f8f9-4094-872a-150db4e9b724",
   "metadata": {},
   "source": [
    "The comparison between the Vector Space Model (VSM) using TF-IDF with cosine similarity and the BM25 algorithm for information retrieval on the 20 Newsgroups dataset reveals insights into their relative strengths. VSM, simpler to implement, performs well in many scenarios but lacks effective document length normalization. BM25, incorporating document length normalization and term saturation, may outperform VSM, especially for longer documents. The choice between these models depends on specific requirements, dataset characteristics, and query types. While our experiment used a single query and a subset of data, it provides a valuable baseline for understanding these algorithms' performance in text classification and document retrieval tasks. Future work could involve testing with larger datasets, multiple queries, and exploring other retrieval models to gain more comprehensive insights into their effectiveness in various information retrieval scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c7e0f-bfc4-4f83-b8cd-09825d2bb4ba",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>ASSESSMENT</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759ecfd-0552-401d-ad9c-09e6f26a8ed3",
   "metadata": {},
   "source": [
    "<img src=\"./marks_distribution.png\" style=\"width: 100%;\" alt=\"marks_distribution\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
