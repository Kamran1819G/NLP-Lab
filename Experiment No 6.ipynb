{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eea62c-be6a-45e6-9b69-99cbf7524c6f",
   "metadata": {},
   "source": [
    "![](./lab%20header%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66a82e-3c64-4e56-8d7c-b15835e618a7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h3>Experiment No. 06</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c407c38-a93b-428d-ad91-6865b43970f1",
   "metadata": {},
   "source": [
    "<img src=\"./Student%20Information.png\" style=\"width: 100%;\" alt=\"Student Information\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eedca-8bc9-493b-bfbd-681db5fbb355",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>AIM</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a2524-23ae-4df1-af53-9c834d2f54f4",
   "metadata": {},
   "source": [
    "**Case study of Viterbi Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f1b94-260f-414c-8c1b-0e1b71b99e04",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>Theory/Procedure/Algorithm</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d072d40-3d52-4640-bdb1-b3e185f63f28",
   "metadata": {},
   "source": [
    "In **Natural Language Processing (NLP)**, the Viterbi algorithm is commonly used in tasks like **Part-of-Speech (POS) tagging, speech recognition, and machine translation**, where a sequence of words or sounds needs to be mapped to a sequence of hidden states. For example, in POS tagging, the observations are the words in a sentence, and the hidden states are the possible tags (e.g., noun, verb, adjective) for each word. The goal is to find the most likely sequence of tags for a given sentence.\n",
    "\n",
    "The underlying model that the Viterbi algorithm works with is a `Hidden Markov Model (HMM)`. In this context:\n",
    "\n",
    "- **States (S)** represent the possible tags or hidden variables (e.g., POS tags: noun, verb, etc.).\n",
    "- **Observations (O)** represent the sequence of words in the sentence.\n",
    "- **Transition Probabilities (A)** represent the likelihood of transitioning from one tag to another (e.g., the probability of a noun being followed by a verb).\n",
    "- **Emission Probabilities (B)** represent the likelihood of a particular word being generated by a specific tag (e.g., the probability that the word \"dog\" is a noun).\n",
    "- **Initial Probabilities (π)** represent the likelihood of starting the sentence with a particular tag (e.g., the probability that the first word is a noun).\n",
    "\n",
    "The Viterbi algorithm finds the most probable sequence of tags (states) for a sequence of words (observations) using dynamic programming. This is essential for tasks such as **POS tagging**, where we need to determine the most likely grammatical structure of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df43de-c629-403a-8005-2959d66b3a8b",
   "metadata": {},
   "source": [
    "##### Steps of the Viterbi Algorithm in NLP:\n",
    "\n",
    "1. **Initialization**: Initialize the probabilities for the first word in the sequence based on the initial probabilities and emission probabilities.\n",
    "   $$\n",
    "   V_1(i) = \\pi(i) \\cdot B_i(o_1)\n",
    "   $$\n",
    "   where $ V_1(i) $ is the probability of being in state $ i $ (e.g., a noun) at the first word and observing $ o_1 $ (the first word).\n",
    "\n",
    "2. **Recursion**: For each subsequent word, compute the maximum probability for each possible tag based on the previous word's tag probabilities and the transition probabilities between tags.\n",
    "   $$\n",
    "   V_t(j) = \\max_{i} \\left[ V_{t-1}(i) \\cdot A_{ij} \\right] \\cdot B_j(o_t)\n",
    "   $$\n",
    "   where $ V_t(j) $ is the maximum probability of being in state $ j $ (e.g., a verb) at time $ t $ and observing $ o_t $ (the word at time $ t $).\n",
    "\n",
    "4. **Termination**: After processing all words, the algorithm identifies the most probable final tag and its probability:\n",
    "   $$\n",
    "   P^* = \\max_i \\left[ V_T(i) \\right]\n",
    "   $$\n",
    "   where $ P^* $ is the highest probability of the final tag.\n",
    "\n",
    "5. **Path Backtracking**: The final step is to backtrack through the matrix to find the most likely sequence of tags for the entire sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf85d8d-36e8-4fb2-8318-9c583b0876f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best POS Path (Tags): [0, 1, 2]\n",
      "Max Probability: 0.0189\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_prob, trans_prob, emis_prob):\n",
    "    # Initialize variables\n",
    "    T = len(observations)  # Number of observations (words)\n",
    "    N = len(states)  # Number of states (POS tags)\n",
    "    \n",
    "    # Initialize dynamic programming table and path table\n",
    "    V = np.zeros((N, T))  # Stores maximum probabilities\n",
    "    path = np.zeros((N, T), dtype=int)  # Stores backtracking paths\n",
    "    \n",
    "    # Initialization step\n",
    "    for i in range(N):\n",
    "        V[i, 0] = start_prob[i] * emis_prob[i, observations[0]]\n",
    "        path[i, 0] = 0\n",
    "    \n",
    "    # Recursion step\n",
    "    for t in range(1, T):\n",
    "        for j in range(N):\n",
    "            max_prob = float('-inf')\n",
    "            max_state = 0\n",
    "            for i in range(N):\n",
    "                prob = V[i, t - 1] * trans_prob[i, j] * emis_prob[j, observations[t]]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = i\n",
    "            V[j, t] = max_prob\n",
    "            path[j, t] = max_state\n",
    "    \n",
    "    # Termination step\n",
    "    max_prob = float('-inf')\n",
    "    last_state = 0\n",
    "    for i in range(N):\n",
    "        if V[i, T - 1] > max_prob:\n",
    "            max_prob = V[i, T - 1]\n",
    "            last_state = i\n",
    "    \n",
    "    # Backtracking to find the best state sequence\n",
    "    best_path = [last_state]\n",
    "    for t in range(T - 1, 0, -1):\n",
    "        best_path.insert(0, path[best_path[0], t])\n",
    "    \n",
    "    return best_path, max_prob\n",
    "\n",
    "# Example case study in NLP (POS tagging)\n",
    "# Observations (words in a sentence)\n",
    "observations = [0, 1, 2]  # Words encoded as indices: \"dog\", \"barks\", \"loudly\"\n",
    "# Hidden states (POS tags: 0 = noun, 1 = verb, 2 = adverb)\n",
    "states = [0, 1, 2]  \n",
    "\n",
    "# Initial probabilities for each POS tag\n",
    "start_prob = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "# Transition probabilities between POS tags\n",
    "trans_prob = np.array([[0.4, 0.5, 0.1],  # noun -> noun, verb, adverb\n",
    "                       [0.3, 0.4, 0.3],  # verb -> noun, verb, adverb\n",
    "                       [0.1, 0.3, 0.6]])  # adverb -> noun, verb, adverb\n",
    "\n",
    "# Emission probabilities (word given POS tag)\n",
    "emis_prob = np.array([[0.6, 0.2, 0.2],  # noun -> \"dog\", \"barks\", \"loudly\"\n",
    "                      [0.1, 0.7, 0.2],  # verb -> \"dog\", \"barks\", \"loudly\"\n",
    "                      [0.1, 0.3, 0.6]])  # adverb -> \"dog\", \"barks\", \"loudly\"\n",
    "\n",
    "# Running Viterbi algorithm\n",
    "best_path, max_prob = viterbi_algorithm(observations, states, start_prob, trans_prob, emis_prob)\n",
    "\n",
    "print(\"Best POS Path (Tags):\", best_path)\n",
    "print(\"Max Probability:\", max_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccad6e2-5535-4bff-9609-eaa218afbeca",
   "metadata": {},
   "source": [
    "##### Explanation of Code:\n",
    "1. **Observations**: In this example, the sentence is encoded as `[0, 1, 2]`, which corresponds to the words \"dog\", \"barks\", and \"loudly\". Each word is represented as an index for simplicity.\n",
    "2. **States**: The possible POS tags are represented as `[0, 1, 2]`, where 0 represents a noun, 1 represents a verb, and 2 represents an adverb.\n",
    "Start Probabilities: The likelihood that the first word of the sentence is a noun, verb, or adverb is given by `start_prob`.\n",
    "3. **Transition Probabilities**: These describe the probability of transitioning from one POS tag to another. For example, the probability of a noun being followed by a verb is higher than that of a noun being followed by an adverb.\n",
    "4. **Emission Probabilities**: These describe how likely it is to observe a word given a particular POS tag. For example, \"dog\" is more likely to be tagged as a noun, \"barks\" as a verb, and \"loudly\" as an adverb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03467912-d373-47e0-bfb5-eb579163224b",
   "metadata": {},
   "source": [
    "##### Example Case:\n",
    "For the sentence \"dog barks loudly\", the Viterbi algorithm finds the most likely sequence of POS tags:\n",
    "\n",
    "- **Best POS Path**: `[0, 1, 2]` — corresponding to noun, verb, adverb.\n",
    "- **Max Probability**: The probability of this sequence of tags given the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab992f17-f9a0-477d-a948-d1307e838670",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>CONCLUSION</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2814c1-f8f9-4094-872a-150db4e9b724",
   "metadata": {},
   "source": [
    "In NLP, the Viterbi algorithm plays a crucial role in sequence labeling tasks like POS tagging. It efficiently finds the most likely sequence of hidden states (e.g., POS tags) given a sequence of observations (e.g., words in a sentence). This approach is particularly useful in real-world applications such as speech recognition and machine translation, where the correct labeling of words is essential for accurate interpretation and generation of language.\n",
    "\n",
    "By using the Viterbi algorithm, we can decode the most probable structure of a sentence in a computationally efficient way, even when faced with a large number of possible states or tags. This case study highlights the practical utility of the Viterbi algorithm in solving complex problems in language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c7e0f-bfc4-4f83-b8cd-09825d2bb4ba",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>ASSESSMENT</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759ecfd-0552-401d-ad9c-09e6f26a8ed3",
   "metadata": {},
   "source": [
    "<img src=\"./marks_distribution.png\" style=\"width: 100%;\" alt=\"marks_distribution\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
