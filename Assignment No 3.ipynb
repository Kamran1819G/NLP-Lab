{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./lab%20header%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h3>Assignment No. 03</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Student%20Information.png\" style=\"width: 100%;\" alt=\"Student Information\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: start;\">\n",
    "    <strong>Q. How can we implement and evaluate different text retrieval models, including Boolean Retrieval, Vector Space Model (TF-IDF), and BM25, from scratch, and compare their performance using common evaluation metrics such as precision, recall, and F1-score?</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text retrieval models are designed to find relevant documents based on a user query from a collection of documents. Different retrieval models approach this task in unique ways. In this assignment, we will focus on three models: Boolean Retrieval, Vector Space Model (TF-IDF), and BM25. We will also evaluate their performance using precision, recall, and F1-score.\n",
    "\n",
    "##### 1. Boolean Retrieval Model (BRM):\n",
    "The Boolean Retrieval Model is the simplest text retrieval model, where documents are retrieved based on exact matches with the query terms using Boolean logic (AND, OR, NOT). This model treats terms as binary (either present or absent) without considering term frequency or relevance.\n",
    "\n",
    "    Pros: Fast and easy to implement.\n",
    "    Cons: No ranking, only exact matches; no consideration of term importance.\n",
    "\n",
    "##### 3. Vector Space Model (TF-IDF):\n",
    "The Vector Space Model represents documents and queries as vectors in a high-dimensional space where each dimension corresponds to a unique term. The TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme is used to assign importance to terms.\n",
    "\n",
    "    - Term Frequency (TF): Measures how frequently a term appears in a document.\n",
    "    - Inverse Document Frequency (IDF): Reduces the weight of terms that appear in many documents, giving more importance to rare terms.\n",
    "\n",
    "Documents are ranked by calculating the cosine similarity between the query vector and document vectors.\n",
    "\n",
    "    Pros: Considers term importance and relevance; ranks documents based on similarity.\n",
    "    Cons: Assumes term independence and linear relationship.\n",
    "\n",
    "##### 3. BM25:\n",
    "BM25 (Best Matching 25) is a probabilistic model that extends TF-IDF by normalizing term frequency using document length and adding tunable parameters. BM25 uses a more sophisticated weighting function than traditional TF-IDF, making it more effective for ranked retrieval.\n",
    "\n",
    "    Pros: More accurate than TF-IDF due to better handling of term saturation and document length normalization.\n",
    "    Cons: More complex; requires parameter tuning.\n",
    "\n",
    "##### 4. Evaluation Metrics:\n",
    "To evaluate the retrieval models, we use the following metrics:\n",
    "\n",
    "    Precision: The proportion of retrieved documents that are relevant.\n",
    "    Recall: The proportion of relevant documents that were retrieved.\n",
    "    F1-Score: The harmonic mean of precision and recall, balancing both metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation: \n",
    "\n",
    "##### 1. Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def tokenize(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Boolean Retrieval Model (BRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_boolean_index(documents):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        tokens = set(tokenize(document))\n",
    "        for token in tokens:\n",
    "            inverted_index[token].add(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "def boolean_search(query, boolean_index):\n",
    "    query_tokens = tokenize(query)\n",
    "    result_set = set()\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in boolean_index:\n",
    "            result_set.update(boolean_index[token])\n",
    "    \n",
    "    return result_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Vector Space Model (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(documents):\n",
    "    tf = []\n",
    "    for document in documents:\n",
    "        doc_tokens = tokenize(document)\n",
    "        doc_tf = defaultdict(float)\n",
    "        for token in doc_tokens:\n",
    "            doc_tf[token] += 1\n",
    "        for token in doc_tf:\n",
    "            doc_tf[token] /= len(doc_tokens)\n",
    "        tf.append(doc_tf)\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    idf = defaultdict(float)\n",
    "    all_tokens = set(token for document in documents for token in tokenize(document))\n",
    "    for token in all_tokens:\n",
    "        df = sum(1 for doc in documents if token in tokenize(doc))\n",
    "        idf[token] = math.log(N / (df + 1))\n",
    "    return idf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = []\n",
    "    for doc_tf in tf:\n",
    "        doc_tf_idf = {}\n",
    "        for token, tf_value in doc_tf.items():\n",
    "            doc_tf_idf[token] = tf_value * idf.get(token, 0)\n",
    "        tf_idf.append(doc_tf_idf)\n",
    "    return tf_idf\n",
    "\n",
    "def cosine_similarity(doc_vector, query_vector):\n",
    "    dot_product = sum(doc_vector.get(token, 0) * query_vector.get(token, 0) for token in query_vector)\n",
    "    doc_norm = math.sqrt(sum(value**2 for value in doc_vector.values()))\n",
    "    query_norm = math.sqrt(sum(value**2 for value in query_vector.values()))\n",
    "    if doc_norm * query_norm == 0:\n",
    "        return 0\n",
    "    return dot_product / (doc_norm * query_norm)\n",
    "\n",
    "def tf_idf_search(query, tf_idf, idf):\n",
    "    query_tokens = tokenize(query)\n",
    "    query_vector = defaultdict(float)\n",
    "    for token in query_tokens:\n",
    "        query_vector[token] += 1\n",
    "    for token in query_vector:\n",
    "        query_vector[token] *= idf.get(token, 0)\n",
    "    \n",
    "    results = []\n",
    "    for doc_id, doc_vector in enumerate(tf_idf):\n",
    "        similarity = cosine_similarity(doc_vector, query_vector)\n",
    "        results.append((doc_id, similarity))\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(documents, query, k1=1.5, b=0.75):\n",
    "    N = len(documents)\n",
    "    avg_doc_len = sum(len(tokenize(doc)) for doc in documents) / N\n",
    "    idf = compute_idf(documents)\n",
    "    \n",
    "    results = []\n",
    "    query_tokens = tokenize(query)\n",
    "    \n",
    "    for doc_id, document in enumerate(documents):\n",
    "        score = 0\n",
    "        doc_tokens = tokenize(document)\n",
    "        doc_len = len(doc_tokens)\n",
    "        doc_tf = defaultdict(float)\n",
    "        for token in doc_tokens:\n",
    "            doc_tf[token] += 1\n",
    "        \n",
    "        for token in query_tokens:\n",
    "            if token in doc_tf:\n",
    "                tf = doc_tf[token]\n",
    "                idf_value = idf.get(token, 0)\n",
    "                term_score = idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_doc_len))))\n",
    "                score += term_score\n",
    "        \n",
    "        results.append((doc_id, score))\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(retrieved_docs, relevant_docs):\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "\n",
    "    true_positives = len(retrieved_set.intersection(relevant_set))\n",
    "    precision = true_positives / len(retrieved_set) if retrieved_set else 0\n",
    "    recall = true_positives / len(relevant_set) if relevant_set else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Model - Precision: 0.3333333333333333, Recall: 0.5, F1-score: 0.4\n",
      "TF-IDF Model - Precision: 0.5, Recall: 1.0, F1-score: 0.6666666666666666\n",
      "BM25 Model - Precision: 0.5, Recall: 1.0, F1-score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Remove non-alphanumeric characters and convert text to lowercase\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Information retrieval models compare different approaches.\",\n",
    "    \"We implement a Boolean retrieval model.\",\n",
    "    \"BM25 is an extension of TF-IDF.\",\n",
    "    \"Text retrieval systems are evaluated using metrics.\"\n",
    "]\n",
    "\n",
    "# Relevant documents for a query\n",
    "relevant_docs = [0, 2]\n",
    "\n",
    "# Query\n",
    "query = \"retrieval model\"\n",
    "\n",
    "# Boolean Retrieval\n",
    "boolean_index = build_boolean_index(documents)\n",
    "boolean_results = boolean_search(query, boolean_index)\n",
    "\n",
    "# Evaluate Boolean Model\n",
    "precision, recall, f1 = precision_recall_f1(boolean_results, relevant_docs)\n",
    "print(f\"Boolean Model - Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "\n",
    "# TF-IDF Retrieval\n",
    "tf = compute_tf(documents)\n",
    "idf = compute_idf(documents)\n",
    "tf_idf = compute_tf_idf(tf, idf)\n",
    "tfidf_results = tf_idf_search(query, tf_idf, idf)\n",
    "\n",
    "# Get top-ranked document IDs for TF-IDF\n",
    "retrieved_tf_idf = [doc_id for doc_id, _ in tfidf_results]\n",
    "\n",
    "# Evaluate TF-IDF Model\n",
    "precision, recall, f1 = precision_recall_f1(retrieved_tf_idf, relevant_docs)\n",
    "print(f\"TF-IDF Model - Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "\n",
    "# BM25 Retrieval\n",
    "bm25_results = bm25(documents, query)\n",
    "\n",
    "# Get top-ranked document IDs for BM25\n",
    "retrieved_bm25 = [doc_id for doc_id, _ in bm25_results]\n",
    "\n",
    "# Evaluate BM25 Model\n",
    "precision, recall, f1 = precision_recall_f1(retrieved_bm25, relevant_docs)\n",
    "print(f\"BM25 Model - Precision: {precision}, Recall: {recall}, F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; border: 1px solid black; display: inline-block; padding: 10px; text-align: center\">\n",
    "    <br>\n",
    "    <br>\n",
    "    <span style=\"font-weight: bold;\">Signature of Lab Incharge</span>\n",
    "    <br>\n",
    "    <span>(Prof. Rupali Sharma)</span> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
