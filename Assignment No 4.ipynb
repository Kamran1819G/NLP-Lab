{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./lab%20header%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h3>Assignment No. 04</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Student%20Information.png\" style=\"width: 100%;\" alt=\"Student Information\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: start;\">\n",
    "    <strong>Q. Implement advanced information retrieval techniques by developing and evaluating Learning to Rank (LTR) models using machine learning methods. Your task is to rank documents based on their relevance to a given query, leveraging features such as term frequency, document length, and other query-document characteristics. Evaluate the performance of the developed models using standard evaluation metrics such as Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR). Document the process, including feature extraction, model training, and evaluation results, and compare the performance of different LTR models.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to Rank (LTR) refers to the application of machine learning to build models that rank items (in this case, documents) based on their relevance to a particular query. It is widely used in information retrieval systems like search engines, recommendation systems, and question-answering systems.\n",
    "\n",
    "LTR models aim to order a set of documents such that the most relevant documents appear higher in the ranked list for a given query. These models are trained on labeled data, where the relevance of documents is provided.\n",
    "\n",
    "LTR methods can be broadly classified into three categories:\n",
    "\n",
    "**1. Pointwise Approach**: In this method, the model learns to predict the relevance score of each document independently. The predicted score is then used to rank the documents.\n",
    "\n",
    "**2. Pairwise Approach**: In the pairwise approach, the model learns to compare pairs of documents and determine which one is more relevant to a query. The model focuses on minimizing the number of incorrectly ordered pairs.\n",
    "\n",
    "**3. Listwise Approach**: Here, the model directly optimizes the ranking of a list of documents. It evaluates the entire list at once, using ranking metrics such as NDCG.\n",
    "\n",
    "#### Features for Learning to Rank\n",
    "Typical features for ranking documents include:\n",
    "\n",
    "- **Term Frequency (TF)**: Frequency of a query term in the document.\n",
    "- **Inverse Document Frequency (IDF)**: Importance of a term across all documents.\n",
    "- **Document Length**: Total number of terms in a document.\n",
    "- **BM25 Score**: A popular ranking function based on term frequency and document length.\n",
    "- **Query-Document Features**: Measures like the number of query terms found in the document, and the proximity of query terms.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "- **Normalized Discounted Cumulative Gain (NDCG)**: Measures how well a ranking matches the ideal order. It discounts the relevance score based on the position of the document in the ranking. Higher ranks contribute more to the score.\n",
    "\n",
    "- **Mean Reciprocal Rank (MRR)**: Measures the rank of the first relevant document in the result set. The reciprocal rank of a query is the inverse of the rank at which the first relevant document is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score: 0.2303\n",
      "MRR Score: 0.4007\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "# Generating synthetic data\n",
    "def generate_synthetic_data(num_queries=1000, num_docs_per_query=10):\n",
    "    np.random.seed(42)\n",
    "    data = []\n",
    "    for qid in range(num_queries):\n",
    "        for did in range(num_docs_per_query):\n",
    "            tf = np.random.randint(1, 100)  # Term Frequency\n",
    "            doc_len = np.random.randint(100, 1000)  # Document Length\n",
    "            relevance = np.random.choice([0, 1, 2], p=[0.7, 0.2, 0.1])  # Relevance score\n",
    "            feature_vector = [tf, doc_len, tf / doc_len]  # Features\n",
    "            data.append([qid] + feature_vector + [relevance])\n",
    "    \n",
    "    columns = ['qid', 'term_frequency', 'doc_length', 'tf_doc_len_ratio', 'relevance']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Load synthetic data\n",
    "df = generate_synthetic_data()\n",
    "\n",
    "# Split features and labels\n",
    "X = df[['term_frequency', 'doc_length', 'tf_doc_len_ratio']].values\n",
    "y = df['relevance'].values\n",
    "qid = df['qid'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, qid_train, qid_test = train_test_split(X, y, qid, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the data for XGBoost\n",
    "train_group = np.unique(qid_train, return_counts=True)[1]\n",
    "test_group = np.unique(qid_test, return_counts=True)[1]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtrain.set_group(train_group)\n",
    "\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "dtest.set_group(test_group)\n",
    "\n",
    "# Parameters for Learning to Rank\n",
    "params = {\n",
    "    'objective': 'rank:pairwise',  # Pairwise ranking objective\n",
    "    'eta': 0.1,  # Learning rate\n",
    "    'gamma': 1.0,  # Minimum loss reduction\n",
    "    'min_child_weight': 0.1,  # Minimum sum of instance weight (hessian)\n",
    "    'max_depth': 6,  # Maximum depth of the trees\n",
    "    'eval_metric': 'ndcg',  # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "rank_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rank_model.predict(dtest)\n",
    "\n",
    "# Evaluate using NDCG and MRR\n",
    "def calculate_mrr(y_true, y_pred, qid):\n",
    "    reciprocal_ranks = []\n",
    "    for q in np.unique(qid):\n",
    "        true_relevance = y_true[qid == q]\n",
    "        predicted_scores = y_pred[qid == q]\n",
    "        ranked_indices = np.argsort(predicted_scores)[::-1]\n",
    "        sorted_true_relevance = true_relevance[ranked_indices]\n",
    "        relevant_doc_indices = np.where(sorted_true_relevance > 0)[0]\n",
    "        if len(relevant_doc_indices) > 0:\n",
    "            reciprocal_ranks.append(1 / (relevant_doc_indices[0] + 1))\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "# NDCG evaluation\n",
    "ndcg_score_value = ndcg_score([y_test], [y_pred], k=10)\n",
    "print(f\"NDCG Score: {ndcg_score_value:.4f}\")\n",
    "\n",
    "# MRR evaluation\n",
    "mrr_score_value = calculate_mrr(y_test, y_pred, qid_test)\n",
    "print(f\"MRR Score: {mrr_score_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "**1. Data Generation**: We simulate a dataset with query IDs and document features such as term frequency, document length, and relevance. Each query is associated with multiple documents.\n",
    "\n",
    "**2. Model Training**: We train an XGBoost model using the pairwise ranking objective `(rank:pairwise)`. XGBoost is suitable for LTR as it supports ranking objectives.\n",
    "\n",
    "**3. Evaluation**:\n",
    "\n",
    "    - We use the NDCG metric to evaluate the quality of the ranked list.\n",
    "    - We compute MRR by evaluating the position of the first relevant document in the ranked list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; border: 1px solid black; display: inline-block; padding: 10px; text-align: center\">\n",
    "    <br>\n",
    "    <br>\n",
    "    <span style=\"font-weight: bold;\">Signature of Lab Incharge</span>\n",
    "    <br>\n",
    "    <span>(Prof. Rupali Sharma)</span> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
