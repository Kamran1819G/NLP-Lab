{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./lab%20header%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h3>Assignment No. 02</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Student%20Information.png\" style=\"width: 100%;\" alt=\"Student Information\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: start;\">\n",
    "    <strong>Q. How can we implement a basic search engine from scratch using Python that tokenizes text, builds an inverted index, and retrieves relevant documents based on search queries?</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A search engine is a system designed to retrieve relevant documents from a collection based on user queries. At its core, a search engine operates by creating an index of the documents, processing user input (queries), and retrieving relevant documents based on those queries. Here's how each component works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Text Tokenization:**\n",
    "Tokenization is the process of breaking down text into smaller units, typically words or terms. This allows the search engine to handle documents and queries in a structured way. Common tokenization techniques include splitting text by spaces and punctuation, removing stop words, and converting to lowercase to ensure uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Remove non-alphanumeric characters and convert text to lowercase\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Inverted Index:**\n",
    "An inverted index is a data structure used to map terms (tokens) to the documents in which they appear. This allows the search engine to quickly retrieve all documents containing a given term. The index is structured as a dictionary, where each term points to a list of document IDs (or other identifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "    \n",
    "    for doc_id, document in enumerate(documents):\n",
    "        tokens = tokenize(document)\n",
    "        for token in tokens:\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(doc_id)\n",
    "    \n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Search and Retrieval:**\n",
    "When a user enters a query, the search engine tokenizes the query, retrieves documents from the inverted index that contain any of the query terms, and ranks them based on relevance. Simple ranking can be done by counting the number of matching terms (term frequency) or by more advanced techniques like TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents matching 'sample document': {0, 1}\n"
     ]
    }
   ],
   "source": [
    "def search(query, inverted_index):\n",
    "    query_tokens = tokenize(query)\n",
    "    matching_documents = set()\n",
    "\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            if not matching_documents:\n",
    "                matching_documents = set(inverted_index[token])\n",
    "            else:\n",
    "                matching_documents.intersection_update(inverted_index[token])\n",
    "    \n",
    "    return matching_documents\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Hello World! This is a sample document.\",\n",
    "    \"This is another sample document.\",\n",
    "    \"Sample search engine implementation using Python.\"\n",
    "]\n",
    "\n",
    "# Build the inverted index\n",
    "inverted_index = build_inverted_index(documents)\n",
    "\n",
    "# Search query\n",
    "query = \"sample document\"\n",
    "results = search(query, inverted_index)\n",
    "\n",
    "# Output search results\n",
    "print(f\"Documents matching '{query}': {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation:\n",
    "- **Tokenization**: Text is tokenized into lowercase words without special characters or punctuation.\n",
    "- **Inverted Index**: The index maps tokens to a list of document IDs where they appear.\n",
    "- **Search**: The query is tokenized, and the search engine finds documents containing the query terms. The search results return the document IDs where all query terms are found.\n",
    "\n",
    "##### Enhancements:\n",
    "- **Stopword Removal**: Exclude common words like 'the', 'is', 'and', etc., to improve search relevance.\n",
    "- **Ranking**: Use TF-IDF or cosine similarity to rank documents based on query relevance.\n",
    "- **Stemming/Lemmatization**: Apply stemming (e.g., \"running\" â†’ \"run\") or lemmatization for better search accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; border: 1px solid black; display: inline-block; padding: 10px; text-align: center\">\n",
    "    <br>\n",
    "    <br>\n",
    "    <span style=\"font-weight: bold;\">Signature of Lab Incharge</span>\n",
    "    <br>\n",
    "    <span>(Prof. Rupali Sharma)</span> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
