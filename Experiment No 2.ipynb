{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eea62c-be6a-45e6-9b69-99cbf7524c6f",
   "metadata": {},
   "source": [
    "![](./lab%20header%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66a82e-3c64-4e56-8d7c-b15835e618a7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h3>Experiment No. 02</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c407c38-a93b-428d-ad91-6865b43970f1",
   "metadata": {},
   "source": [
    "<img src=\"./Student%20Information.png\" style=\"width: 100%;\" alt=\"Student Information\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eedca-8bc9-493b-bfbd-681db5fbb355",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>AIM</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a2524-23ae-4df1-af53-9c834d2f54f4",
   "metadata": {},
   "source": [
    "**Analysis of natural language using lexical analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f1b94-260f-414c-8c1b-0e1b71b99e04",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>Theory/Procedure/Algorithm</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4b635-f7eb-4375-b738-26fd449365c8",
   "metadata": {},
   "source": [
    "**Lexical Analysis** is the process of converting a sequence of characters into a sequence of tokens. A token is a string with an assigned meaning, which is the basic building block of syntax analysis. In the context of natural language processing (NLP), lexical analysis involves breaking down the text into words, phrases, or symbols and categorizing them into predefined classes like keywords, operators, identifiers, etc.\n",
    "\n",
    "The key steps in lexical analysis are:\n",
    "\n",
    "1. `Tokenization`: Splitting the input text into individual tokens (words, punctuation, etc.).\n",
    "2. `Normalization`: Transforming tokens into a standard form (e.g., converting all text to lowercase).\n",
    "3. `Classification`: Assigning each token to a category such as noun, verb, adjective, etc.\n",
    "4. `Handling Punctuation`: Differentiating between words and punctuation marks to ensure accurate analysis.\n",
    "\n",
    "Lexical analysis is critical in natural language processing tasks such as text parsing, speech recognition, and data extraction. By analyzing the structure of language at the lexical level, one can gain insights into the syntax and semantics of the text, enabling more sophisticated language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3de4e5-d3aa-43d8-abc5-04af96fe7560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'dog', 'amused', 'fox', \"'s\", 'antics']\n",
      "Lemmas: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'dog', 'amused', 'fox', \"'s\", 'antic']\n",
      "POS Tags: [('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('lazy', 'JJ'), ('dog', 'NN'), ('dog', 'NN'), ('amused', 'VBD'), ('fox', 'NN'), (\"'s\", 'POS'), ('antics', 'NNS')]\n",
      "Top 10 Word Frequencies: {'fox': 2, 'dog': 2, 'quick': 1, 'brown': 1, 'jump': 1, 'lazy': 1, 'amused': 1, \"'s\": 1, 'antic': 1}\n",
      "Lexical Diversity: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def download_nltk_resources():\n",
    "    resources = ['punkt', 'averaged_perceptron_tagger', 'wordnet', 'stopwords']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {resource}: {str(e)}\")\n",
    "            print(f\"You may need to manually download this resource.\")\n",
    "\n",
    "download_nltk_resources()\n",
    "\n",
    "def lexical_analysis(text):\n",
    "    try:\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Lowercasing\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        # Remove punctuation and numbers\n",
    "        tokens = [token for token in tokens if token not in string.punctuation and not token.isnumeric()]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Part-of-speech tagging\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        \n",
    "        # Word frequency\n",
    "        freq_dist = FreqDist(lemmas)\n",
    "        \n",
    "        # Lexical diversity (unique lemmas / total lemmas)\n",
    "        lexical_diversity = len(set(lemmas)) / len(lemmas) if lemmas else 0\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'lemmas': lemmas,\n",
    "            'pos_tags': pos_tags,\n",
    "            'word_frequencies': dict(freq_dist.most_common(10)),  # Top 10 most common words\n",
    "            'lexical_diversity': lexical_diversity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. The dog was not amused by the fox's antics.\"\n",
    "result = lexical_analysis(sample_text)\n",
    "\n",
    "if result:\n",
    "    print(\"Tokens:\", result['tokens'])\n",
    "    print(\"Lemmas:\", result['lemmas'])\n",
    "    print(\"POS Tags:\", result['pos_tags'])\n",
    "    print(\"Top 10 Word Frequencies:\", result['word_frequencies'])\n",
    "    print(\"Lexical Diversity:\", result['lexical_diversity'])\n",
    "else:\n",
    "    print(\"Analysis could not be completed due to an error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab992f17-f9a0-477d-a948-d1307e838670",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>CONCLUSION</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2814c1-f8f9-4094-872a-150db4e9b724",
   "metadata": {},
   "source": [
    "Lexical analysis plays a crucial role in understanding and processing natural language. By breaking down text into tokens and analyzing their distribution, we can gain valuable insights into the language's structure and semantics. The simple lexical analysis performed in this experiment demonstrates how natural language can be dissected and analyzed to uncover patterns, frequencies, and relationships within the text. This foundational step is essential in more advanced NLP tasks such as parsing, sentiment analysis, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c7e0f-bfc4-4f83-b8cd-09825d2bb4ba",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: center;\">\n",
    "    <strong>ASSESSMENT</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759ecfd-0552-401d-ad9c-09e6f26a8ed3",
   "metadata": {},
   "source": [
    "<img src=\"./marks_distribution.png\" style=\"width: 100%;\" alt=\"marks_distribution\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
